{%- from 'hadoop/settings.sls' import hadoop with context %}
{%- from 'hadoop/hdfs/settings.sls' import hdfs with context %}
{%- set dyn_cfg = hdfs.get('config_hdfs_site', {}) %}
{%- set major = hadoop.major_version|string() -%}
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
{%- if hdfs.namenode_count > 1 %}
  {%- from 'zookeeper/settings.sls' import zk with context %}
{%- else %}
  {%- set zk = {} %}
{%- endif %}

<configuration>
    <property>
        <name>{{ 'dfs.name.dir' if major == '1' else 'dfs.namenode.name.dir' }}</name>
        <value>
{%- for d in hdfs.local_disks -%}
file://{{ d }}/hdfs/nn{{ '' if loop.last else ',' }}
{%- endfor -%}</value>
        <final>true</final>
    </property>

    <property>
        <name>{{ 'dfs.data.dir' if major == '1' else 'dfs.datanode.data.dir' }}</name>
        <value>
{%- for d in hdfs.local_disks -%}
file://{{ d }}/hdfs/dn{{ '' if loop.last else ',' }}
{%- endfor -%}</value>
        <final>true</final>
    </property>

{%- if major == '2' %}
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>
{%- for d in hdfs.local_disks -%}
file://{{ d }}/hdfs/snn{{ '' if loop.last else ',' }}
{%- endfor -%}</value>
        <final>true</final>
    </property>
{%- endif %}

{%- if hdfs.namenode_count == 1 %}
    <property>
        <name>{{ 'dfs.http-address' if major == '1' else 'dfs.namenode.http-address' }}</name>
        <value>{{ hdfs.namenode_host }}:{{ hdfs.namenode_http_port }}</value>
        <final>true</final>
    </property>

    <property>
        <name>{{ 'dfs.secondary.http-address' if major == '1' else 'dfs.secondary.namenode.http-address' }}</name>
        <value>{{ hdfs.namenode_host }}:{{ hdfs.secondarynamenode_http_port }}</value>
        <final>true</final>
    </property>
{%- else %}
    <!-- setting up namenode HA as two namenodes are configured in Salt -->
    <property>
        <name>dfs.nameservices</name>
        <value>{{hdfs.ha_cluster_id}}</value>
    </property>

    <property>
        <name>dfs.ha.namenodes.{{hdfs.ha_cluster_id}}</name>
        <value>
{%- for i in range(hdfs.namenode_count) -%}
nn{{loop.index}}{{ '' if loop.last else ',' }}
{%- endfor -%}</value>
    </property>
{% for namenode in hdfs.namenode_hosts %}
    <property>
        <name>dfs.namenode.rpc-address.{{hdfs.ha_cluster_id}}.nn{{loop.index}}</name>
        <value>{{namenode}}:{{hdfs.namenode_port}}</value>
    </property>
    
    <property>
        <name>dfs.namenode.http-address.{{hdfs.ha_cluster_id}}.nn{{loop.index}}</name>
        <value>{{namenode}}:{{hdfs.namenode_http_port}}</value>
    </property>
{%- endfor -%}
    
    <!-- todo: setup https? -->

    <!-- doc says:
         You may only use a single path for this configuration.
         Redundancy for this data is provided by running multiple separate JournalNodes,
         or by configuring this directory on a locally-attached RAID array

         Also: (not quite documented) you cannot use a file:// url for this path 
       -->
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>{{ hdfs.local_disks|first() }}/hdfs/journal</value>
        <final>true</final>
    </property>

    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://{{hdfs.quorum_connection_string}}/{{hdfs.ha_cluster_id}}</value>
    </property>

    <property>
        <name>dfs.client.failover.proxy.provider.{{hdfs.ha_cluster_id}}</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

{%- if zk %}
    <!-- zookeeper and fencing configuration -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>ha.zookeeper.quorum</name>
        <value>{{ zk.connection_string }}</value>
    </property>

{%- if dyn_cfg['dfs.ha.fencing.methods'] is not defined %}
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>

    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/hdfs/.ssh/id_dsa</value>
    </property>
{%- endif %}

{%- endif %}
    <!-- end of HA attributes section -->
{%- endif %}

    <property>
        <name>dfs.hosts</name>
        <value>{{ hadoop.alt_config }}/dfs.hosts</value>
    </property>

    <property>
        <name>dfs.hosts.exclude</name>
        <value>{{ hadoop.alt_config }}/dfs.hosts.exclude</value>
    </property>

    <property>
        <name>dfs.replication</name>
        <value>{{ hdfs.replicas }}</value>
    </property>

{%- for name, subdict in dyn_cfg.items() %}
    <property>
        <name>{{ name }}</name>
{%- for k,v in subdict.items() %}
        <{{k}}>{{ v }}</{{k}}>
{%- endfor %}
    </property>
{%- endfor %}
</configuration>
